diff --git a/src/python/PSetTweaks/WMTweak.py b/src/python/PSetTweaks/WMTweak.py
index 6c40309..d80ac18 100644
--- a/src/python/PSetTweaks/WMTweak.py
+++ b/src/python/PSetTweaks/WMTweak.py
@@ -430,15 +430,17 @@ def makeJobTweak(job):
         result.addParameter("process.source.fileNames", primaryFiles)
         if len(secondaryFiles) > 0:
             result.addParameter("process.source.secondaryFileNames", secondaryFiles)
-    elif not lheInput:
+    else:
         #First event parameter should be set from whatever the mask says,
         #That should have the added protection of not going over 2^32 - 1
         #If there is nothing in the mask, then we fallback to the counter method
         if job['mask'].get('FirstEvent',None) != None:
             result.addParameter("process.source.firstEvent",
                                 job['mask']['FirstEvent'])
-        else:
+        elif not lheInput:
             #No first event information in the mask, raise and error
+            #
+            #Don't raise an error for LHE backwards compatibility
             raise WMTweakMaskError(job['mask'],
                                    "No first event information provided in the mask")
 
diff --git a/src/python/WMCore/DataStructs/Mask.py b/src/python/WMCore/DataStructs/Mask.py
index 0d3485d..eee3ea7 100644
--- a/src/python/WMCore/DataStructs/Mask.py
+++ b/src/python/WMCore/DataStructs/Mask.py
@@ -13,7 +13,6 @@ job in two ways:
 import logging
 
 from WMCore.DataStructs.Run import Run
-from WMCore.DataStructs.LumiList import LumiList
 
 class Mask(dict):
     """
@@ -155,17 +154,6 @@ class Mask(dict):
 
         return
 
-    def removeLumiList(self, lumiList):
-        """
-        Remove a lumi list from this data structure
-
-        This requires conversion to LumiList to do the lumi algebra an
-        may be computationally expensive for a large number of lumis.
-        """
-        myLumis = LumiList(compactList=self['runAndLumis'])
-        myLumis = myLumis - lumiList
-        self['runAndLumis'] = myLumis.getCompactList()
-
     def getRunAndLumis(self):
         """
         _getRunAndLumis_
diff --git a/src/python/WMCore/JobSplitting/LumiBased.py b/src/python/WMCore/JobSplitting/LumiBased.py
index 65662bc..bb6fdd0 100644
--- a/src/python/WMCore/JobSplitting/LumiBased.py
+++ b/src/python/WMCore/JobSplitting/LumiBased.py
@@ -16,7 +16,6 @@ import threading
 import traceback
 
 from WMCore.DataStructs.Run import Run
-from WMCore.DataStructs.LumiList import LumiList
 
 from WMCore.JobSplitting.JobFactory import JobFactory
 from WMCore.WMBS.File               import File
@@ -60,6 +59,76 @@ def isGoodRun(goodRunList, run):
 
     return False
 
+class LumiChecker:
+    """ Simple utility class that helps correcting dataset that have lumis split across jobs:
+
+        Due to error in processing (in particular, the Run I Tier-0), some
+        lumi sections may be spread across multiple jobs. This class helps keep tracking of these
+        lumis.
+    """
+
+    def __init__(self, applyLumiCorrection):
+        # This is a dictionary that contains (run, lumis) pairs as keys, and job ojects as values
+        # The run/lumi keys are added as soon as the lumi is processed by the splitting algorithm
+        # The job value is added when the newJob method is invoked
+        self.lumiJobs = {}
+        # This dictionary contains (run, lumis) pairs as keys, and a list of files as values
+        # The logic is that as soon as a split lumi is seen we add its input file here
+        self.splitLumiFiles = {}
+        self.applyLumiCorrection = applyLumiCorrection
+
+    def isSplitLumi(self, run, lumi, file_):
+        """ Check if a lumi has already been processed, and return True if it is the case.
+            Also saves the input file containing the lumi if this happens.
+
+            The method adds the (run, lumi) pair key to lumiJobs, and it sets its value to None.
+            This value will be set from None to the job object as soon as the splitting algorithm
+            switch to a new job.
+            If a split lumi is encountered we add its input file to the self.splitLumiFiles dict
+        """
+        if not self.applyLumiCorrection: # if we don't have to apply the correction simply exit
+            return False
+
+        # This means the lumi has already been processed and the job has changed
+        isSplit = (run, lumi) in self.lumiJobs
+
+        if isSplit:
+            self.splitLumiFiles.setdefault((run, lumi), []).append(file_)
+            logging.warning("Skipping runlumi pair (%s, %s) as it was already been processed."
+                            "Will add %s to the input files of the job processing the lumi"
+                                    % (run, lumi, file_['lfn']))
+        else:
+            self.lumiJobs[(run, lumi)] = None
+
+        return isSplit
+
+    def closeJob(self, job):
+        """ Go through the list of lumis of the job and add an entry to "lumiJobs"
+
+            For each (run,lumi) pair in the job I create an entry in the dictionary so we know if the lumi
+            has already been added to another job, and we know to which job (so later we can add files to this
+            job if duplicated lumis are found)
+        """
+        if not self.applyLumiCorrection:
+            return
+        if job: # the first time you call "newJob" in the splitting algorithm currentJob is None
+            for run, lumiIntervals in job['mask']['runAndLumis'].iteritems():
+                for startLumi, endLumi in lumiIntervals:
+                    for lumi in xrange(startLumi, endLumi + 1):
+                        self.lumiJobs[(run, lumi)] = job
+
+    def fixInputFiles(self):
+        """ Called at the end. Iterates over the split lumis, and add their input files to the first job where the lumi
+            was seen.
+        """
+        # Just a cosmetic "if": self.splitLumiFiles is empty when applyLumiCorrection is not enabled
+        if not self.applyLumiCorrection:
+            return
+        for (run, lumi), files in self.splitLumiFiles.iteritems():
+            for file_ in files:
+                self.lumiJobs[(run, lumi)].addFile(file_)
+
+
 
 class LumiBased(JobFactory):
     """
@@ -89,6 +158,7 @@ class LumiBased(JobFactory):
         runs = kwargs.get('runs', None)
         lumis = kwargs.get('lumis', None)
         deterministicPileup = kwargs.get('deterministicPileup', False)
+        applyLumiCorrection = bool(kwargs.get('applyLumiCorrection', False))
         eventsPerLumiInDataset = 0
 
         if deterministicPileup and self.package == 'WMCore.WMBS':
@@ -172,9 +242,6 @@ class LumiBased(JobFactory):
                 newlist.append(f)
             locationDict[key] = sorted(newlist, key = operator.itemgetter('lowestRun'))
 
-
-
-
         # Split files into jobs with each job containing
         # EXACTLY lumisPerJob number of lumis (except for maybe the last one)
 
@@ -186,13 +253,13 @@ class LumiBased(JobFactory):
         lastRun = None
         lumisInJob = 0
         lumisInTask = 0
+        self.lumiChecker = LumiChecker(applyLumiCorrection)
         for location in locationDict.keys():
 
             # For each location, we need a new jobGroup
             self.newGroup()
             stopJob = True
             for f in locationDict[location]:
-
                 if getParents:
                     parentLFNs = self.findParent(lfn = f['lfn'])
                     for lfn in parentLFNs:
@@ -218,7 +285,8 @@ class LumiBased(JobFactory):
 
                     # Now loop over the lumis
                     for lumi in run:
-                        if not isGoodLumi(goodRunList, run = run.run, lumi = lumi):
+                        if (not isGoodLumi(goodRunList, run = run.run, lumi = lumi)
+                                or self.lumiChecker.isSplitLumi(run.run, lumi, f)): # splitLumi checks if the lumi is split across jobs
                             # Kill the chain of good lumis
                             # Skip this lumi
                             if firstLumi != None and firstLumi != lumi:
@@ -262,6 +330,7 @@ class LumiBased(JobFactory):
                                 runAddedSize = addedEvents * sizePerEvent
                                 self.currentJob.addResourceEstimates(jobTime = runAddedTime,
                                                                      disk = runAddedSize)
+                            self.lumiChecker.closeJob(self.currentJob) # before creating a new job add the lumis of the current one to the checker
                             self.newJob(name = self.getJobName())
                             self.currentJob.addResourceEstimates(memory = memoryRequirement)
                             if deterministicPileup:
@@ -303,57 +372,8 @@ class LumiBased(JobFactory):
                 if stopTask:
                     break
 
-            # We now have a list of jobs in a job group.  We will now iterate through them
-            # to verify we have no single lumi processed by multiple jobs.
-            jobs = self.currentGroup.newjobs
-            overlap = 0
-            lumicount = 0
-            logging.debug("Current job group has %d jobs." % len(jobs))
-            for idx1 in xrange(len(jobs)):
-                job1 = jobs[idx1]
-                lumicount += len(set(LumiList(compactList=job1['mask'].getRunAndLumis()).getLumis()))
-                for idx2 in xrange(idx1+1, len(jobs)):
-                    job2 = jobs[idx2]
-                    overlap += self.lumiCorrection(job1, job2, locationDict[location])
-            logging.info("There were %d overlapping lumis and %d total lumis." % (overlap, lumicount))
-
             if stopTask:
                 break
 
+        self.lumiChecker.fixInputFiles()
         return
-
-    def lumiCorrection(self, job1, job2, locations):
-        """
-        Due to error in processing (in particular, the Run I Tier-0), some
-        lumi sections may be spread across multiple jobs.  Where possible:
-         - Remove a lumi from job2 if it is in both job1 and job2
-         - If the lumi is in multiple files and it spanned multiple jobs,
-           make sure that all those files are processed by job1.
-
-        NOTE: This will not help in the case where a lumi is split across
-        multiple blocks.
-
-        Returns the number of affected lumis
-        """
-        lumis1 = LumiList(compactList=job1['mask'].getRunAndLumis())
-        lumis2 = LumiList(compactList=job2['mask'].getRunAndLumis())
-        ilumis = lumis1 & lumis2
-        lumiPairs = ilumis.getLumis()
-        if not lumiPairs:
-            return 0
-        logging.warning("%d lumis appear in multiple jobs: %s" % (len(lumiPairs), str(ilumis)))
-        job2['mask'].removeLumiList(ilumis)
-
-        for run, lumi in lumiPairs:
-            for fileObj in locations:
-                if fileObj in job1['input_files']:
-                    continue
-                for runObj in fileObj['runs']:
-                    if run == runObj.run:
-                        if lumi in runObj.lumis:
-                            if fileObj not in job1['input_files']:
-                                logging.warning("Adding file %s to job input files so it will process all of a lumi section." % fileObj['lfn'])
-                                job1.addFile(fileObj)
-                                break
-
-        return len(lumiPairs)
diff --git a/src/python/WMCore/WMRuntime/Scripts/SetupCMSSWPset.py b/src/python/WMCore/WMRuntime/Scripts/SetupCMSSWPset.py
index 8249eea..e820321 100755
--- a/src/python/WMCore/WMRuntime/Scripts/SetupCMSSWPset.py
+++ b/src/python/WMCore/WMRuntime/Scripts/SetupCMSSWPset.py
@@ -604,7 +604,7 @@ class SetupCMSSWPset(ScriptInterface):
         self.handlePerformanceSettings()
 
         # check for event numbers in the producers
-        self.handleProducersNumberOfEvents()
+#        self.handleProducersNumberOfEvents()
 
         # fixup the dqmFileSaver
         self.handleDQMFileSaver()
diff --git a/test/python/WMCore_t/JobSplitting_t/LumiBased_t.py b/test/python/WMCore_t/JobSplitting_t/LumiBased_t.py
index ec403bd..43d111f 100644
--- a/test/python/WMCore_t/JobSplitting_t/LumiBased_t.py
+++ b/test/python/WMCore_t/JobSplitting_t/LumiBased_t.py
@@ -74,7 +74,7 @@ class LumiBasedTest(unittest.TestCase):
                                events = 100)
                 lumis = []
                 for lumi in range(lumisPerFile):
-                    lumis.append((i * 100) + lumi)
+                    lumis.append(5 + 10 * (i * 100) + lumi) #lumis should be different
                 newFile.addRun(Run(i, *lumis))
                 newFile.setLocation('malpaquet')
                 testFileset.addFile(newFile)
@@ -237,11 +237,13 @@ class LumiBasedTest(unittest.TestCase):
 
         jobFactory = splitter(package = "WMCore.DataStructs",
                               subscription = testSubscription)
-        
+
         jobGroups = jobFactory(lumis_per_job = 3,
                                halt_job_on_file_boundaries = False,
                                splitOnRun = False,
-                               performance = self.performanceParams)
+                               performance = self.performanceParams,
+                               applyLumiCorrection = True
+                              )
 
         self.assertEqual(len(jobGroups), 1)
         jobs = jobGroups[0].jobs
@@ -250,7 +252,30 @@ class LumiBasedTest(unittest.TestCase):
         self.assertEqual(len(jobs[0]['input_files']), 2)
         self.assertEqual(len(jobs[1]['input_files']), 1)
         self.assertEqual(jobs[0]['mask'].getRunAndLumis(), {0: [[0L, 1L], [42L, 42L]]})
-        self.assertEqual(jobs[1]['mask'].getRunAndLumis(), {'0': [[100L, 101L]]})
+        self.assertEqual(jobs[1]['mask'].getRunAndLumis(), {0: [[100L, 101L]]})
+
+        #Test that we are not removing all the lumis from the jobs anymore
+        removedLumi = self.createSubscription(nFiles = 4, lumisPerFile = 1)
+        #Setting the lumi of job 0 to value 100, as the one of job one
+        runObj = next(iter(removedLumi.getFileset().getFiles()[0]['runs']))
+        runObj.run = 1
+        runObj[0] = 100
+        jobFactory = splitter(package = "WMCore.DataStructs",
+                              subscription = removedLumi)
+        jobGroups = jobFactory(lumis_per_job = 1,
+                               halt_job_on_file_boundaries = True,
+                               performance = self.performanceParams,
+                               applyLumiCorrection = True)
+        # we need to end up with 3 jobs and one job with two input files
+        jobs = jobGroups[0].jobs
+
+        self.assertEqual(len(jobs), 3)
+        self.assertEqual(len(jobs[0]['input_files']), 2)
+        self.assertEqual(len(jobs[1]['input_files']), 1)
+        self.assertEqual(len(jobs[2]['input_files']), 1)
+        self.assertEqual(jobs[0]['mask'].getRunAndLumis(), {1: [[100L, 100L]]})
+        self.assertEqual(jobs[1]['mask'].getRunAndLumis(), {2: [[200L, 200L]]})
+        self.assertEqual(jobs[2]['mask'].getRunAndLumis(), {3: [[300L, 300L]]})
 
 if __name__ == '__main__':
     unittest.main()
